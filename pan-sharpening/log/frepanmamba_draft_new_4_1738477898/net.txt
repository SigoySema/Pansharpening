Net(
  (ms_to_token): PatchEmbed(
    (proj): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1))
    (norm): LayerNorm(
      (body): BiasFree_LayerNorm()
    )
  )
  (pan_to_token): PatchEmbed(
    (proj): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))
    (norm): LayerNorm(
      (body): BiasFree_LayerNorm()
    )
  )
  (ms_feature_extraction_level): ModuleList(
    (0-3): 4 x VSSBlock(
      (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (act): SiLU()
        (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
        (dropout): Identity()
      )
      (drop_path): DropPath()
      (ln_11): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (self_attention1): SS2D(
        (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (act): SiLU()
        (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
        (dropout): Identity()
      )
      (drop_path1): DropPath()
      (conv_blk): CAB(
        (cab): Sequential(
          (0): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): ChannelAttention(
            (attention): Sequential(
              (0): AdaptiveAvgPool2d(output_size=1)
              (1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))
              (2): ReLU(inplace=True)
              (3): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
              (4): Sigmoid()
            )
          )
        )
      )
      (ln_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (block): Sequential(
        (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.1, inplace=True)
        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (3): LeakyReLU(negative_slope=0.1, inplace=True)
      )
      (linear_out): Linear(in_features=96, out_features=32, bias=True)
    )
  )
  (pan_feature_extraction_level): ModuleList(
    (0-3): 4 x VSSBlock(
      (ln_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (self_attention): SS2D(
        (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (act): SiLU()
        (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
        (dropout): Identity()
      )
      (drop_path): DropPath()
      (ln_11): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (self_attention1): SS2D(
        (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (in_proj): Linear(in_features=32, out_features=128, bias=False)
        (act): SiLU()
        (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (out_proj): Linear(in_features=64, out_features=32, bias=False)
        (dropout): Identity()
      )
      (drop_path1): DropPath()
      (conv_blk): CAB(
        (cab): Sequential(
          (0): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): GELU(approximate='none')
          (2): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (3): ChannelAttention(
            (attention): Sequential(
              (0): AdaptiveAvgPool2d(output_size=1)
              (1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))
              (2): ReLU(inplace=True)
              (3): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))
              (4): Sigmoid()
            )
          )
        )
      )
      (ln_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
      (block): Sequential(
        (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.1, inplace=True)
        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (3): LeakyReLU(negative_slope=0.1, inplace=True)
      )
      (linear_out): Linear(in_features=96, out_features=32, bias=True)
    )
  )
  (CMF_Mamaba_level0): CMF_Mamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (CMF_Mamaba_level1): CMF_Mamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (CMF_Mamaba_level2): CMF_Mamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (CMF_Mamaba_level3): CMF_Mamba(
    (cross_mamba): Mamba(
      (in_proj): Linear(in_features=32, out_features=128, bias=False)
      (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (act): SiLU()
      (x_proj): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj): Linear(in_features=2, out_features=64, bias=True)
      (conv1d_b): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)
      (x_proj_b): Linear(in_features=64, out_features=34, bias=False)
      (dt_proj_b): Linear(in_features=2, out_features=64, bias=True)
      (in_proj_extra): Linear(in_features=32, out_features=128, bias=False)
      (out_proj): Linear(in_features=64, out_features=32, bias=False)
    )
    (norm1): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (norm2): LayerNorm(
      (body): WithBias_LayerNorm()
    )
    (dwconv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
  )
  (fusion_out): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))
)
